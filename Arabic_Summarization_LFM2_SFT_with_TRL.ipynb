{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3PTFH-H9Ozk"
      },
      "source": [
        "# 💧 LFM2 - SFT with TRL\n",
        "\n",
        "This tutorial demonstrates how to fine-tune our LFM2 models, e.g. [`LiquidAI/LFM2-1.2B`](https://huggingface.co/LiquidAI/LFM2-1.2B), using the TRL library.\n",
        "\n",
        "Follow along if it's your first time using trl, or take single code snippets for your own workflow\n",
        "\n",
        "## 🎯 What you'll find:\n",
        "- **SFT** (Supervised Fine-Tuning) - Basic instruction following\n",
        "- **LoRA + SFT** - Using LoRA (from PEFT) to SFT while on constrained hardware\n",
        "\n",
        "## 📋 Prerequisites:\n",
        "- **GPU Runtime**: Select GPU in `Runtime` → `Change runtime type`\n",
        "- **Hugging Face Account**: For accessing models and datasets\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0RPLu2h9ome"
      },
      "source": [
        "# 📦 Installation & Setup\n",
        "\n",
        "First, let's install all the required packages:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3FIcp_wo9nsR",
        "tags": []
      },
      "outputs": [],
      "source": [
        "!pip install transformers==4.54.1 trl>=0.18.2 peft>=0.15.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8qlnEnVvQFg",
        "outputId": "7ef6c308-43a0-43cc-f12d-c9029d496ad4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install sentencepiece --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "tags": [],
        "id": "P7hBRIbavQFg"
      },
      "outputs": [],
      "source": [
        "#!pip install patchelf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "tags": [],
        "id": "yOkmAj03vQFg"
      },
      "outputs": [],
      "source": [
        "#!patchelf --add-rpath '$ORIGIN/../../nvidia/cusparse/lib' /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41UEf1uxCd6m"
      },
      "source": [
        "Let's now verify the packages are installed correctly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSJgYtHT_Os4",
        "outputId": "75f22ad0-e52f-4643-8949-d1df27c93bf9",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📦 PyTorch version: 2.6.0+cu124\n",
            "🤗 Transformers version: 4.54.1\n",
            "📊 TRL version: 0.20.0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import transformers\n",
        "import trl\n",
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "print(f\"📦 PyTorch version: {torch.__version__}\")\n",
        "print(f\"🤗 Transformers version: {transformers.__version__}\")\n",
        "print(f\"📊 TRL version: {trl.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_uXLzxQ_rnK"
      },
      "source": [
        "# Loading the model from Transformers 🤗\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "iA3erKM4-HhS",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38c56377-ec22-4c58-a18e-b5c745987181"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📚 Loading tokenizer...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧠 Loading model...\n",
            "✅ Local model loaded successfully!\n",
            "🔢 Parameters: 354,483,968\n",
            "📖 Vocab size: 64400\n",
            "💾 Model size: ~0.7 GB (bfloat16)\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from IPython.display import display, HTML, Markdown\n",
        "import torch\n",
        "\n",
        "model_id = \"LiquidAI/LFM2-350M\" # <- or LFM2-700M or LFM2-350M\n",
        "\n",
        "print(\"📚 Loading tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "print(\"🧠 Loading model...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=\"bfloat16\",\n",
        "#   attn_implementation=\"flash_attention_2\" <- uncomment on compatible GPU\n",
        ")\n",
        "\n",
        "print(\"✅ Local model loaded successfully!\")\n",
        "print(f\"🔢 Parameters: {model.num_parameters():,}\")\n",
        "print(f\"📖 Vocab size: {len(tokenizer)}\")\n",
        "print(f\"💾 Model size: ~{model.num_parameters() * 2 / 1e9:.1f} GB (bfloat16)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ABA6Yrm_lql"
      },
      "source": [
        "# 🎯 Part 1: Supervised Fine-Tuning (SFT)\n",
        "\n",
        "SFT teaches the model to follow instructions by training on input-output pairs (instruction vs response). This is the foundation for creating instruction-following models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KufdgeypHtst"
      },
      "source": [
        "## Load an SFT Dataset\n",
        "\n",
        "We will use [HuggingFaceTB/smoltalk](https://huggingface.co/datasets/HuggingFaceTB/smoltalk), limiting ourselves to the first 5k samples for brevity. Feel free to change the limit by changing the slicing index in the parameter `split`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "XCe8O06-_Cps",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b24d106-93ab-46a2-c967-9110d7d07561"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📥 Loading SFT dataset...\n",
            "✅ SFT Dataset loaded:\n",
            "   📚 Train samples: 17862\n",
            "   🧪 Eval samples: 181\n",
            "\n",
            "📝 Single Sample: [{'content': 'Summarize the following text: \\n\\n بقلم: وليام تورفيل. نشر في: 08:04 صباحًا بتوقيت شرق الولايات المتحدة، 21 ديسمبر 2013 | تم تحديثه: 09:07 صباحًا بتوقيت شرق الولايات المتحدة، 21 ديسمبر 2013. إذا كنت تدخل في الروح المناسبة لعيد الميلاد بمجرد سماع أغنية عيد الميلاد أو رؤية صفوف من الزخارف للبيع، فمن الأفضل أن تبتعد عن هذه الشوارع. قد تبدو شوارع مثل تينسيل لين في نونيتون، وارويكشاير، غريبة في الصيف - لكنها تأخذ مكانها في ديسمبر. وجد بحث موظفي رويال ميل أن المملكة المتحدة لديها 3369 اسمًا لشوارع مرتبطة بعيد الميلاد، بناءً على تحليل كل اسم شارع في البلاد. تينسيل لين في نونيتون، وارويكشاير، مزينة هذا العام من قبل السكان ميشيل أورتون وابنها ذي الثلاث سنوات، ماثيو. تعرض الأخوات نيام ديفي (على اليسار)، البالغة من العمر ثماني سنوات، وسيارا ديفي، البالغة من العمر خمس سنوات، أمام اسم شارعهم - الذي قد يبدو غريبًا بعض الشيء في الصيف - في نورثامبتون. هولي ستريت هي أكثر الأسماء شيوعًا لشوارع عيد الميلاد في المملكة المتحدة. هناك 990 اسمًا لهذا النوع من الشوارع في جميع أنحاء المملكة المتحدة. 1. شارع هولي. 2. شارع بيل. 3. شارع ماري. 4. شارع إنجل. 5. طريق ستار. 6. شارع بو. 7. ساحة فيستيفال. 8. شارع نويل. 9. شارع غولد. 10. شارع غارلاند. كشف البحث أيضًا أن إقليم ميدلاندز لديه ما مجموعه 560 اسمًا لشوارع عيد الميلاد، أكثر من أي منطقة أخرى في المملكة المتحدة. تحتوي المنطقة على ما مجموعه 560 اسمًا لشوارع عيد الميلاد وتتمتع بأسماء شوارع مناسبة لعيد الميلاد مثل ريندير رود، ونويل رود، وبيل كلوز، وهولي ستريت. التقييمات جاءت من تحليل قاعدة بيانات رويال ميل التي تحتوي على 29 مليون عنوان في المملكة المتحدة. تكشف قاعدة البيانات أيضًا عن حبنا لرنود سانتا، حيث أن سبعة من تسعة رنود ممثلة في اسم شارع. وتشمل هذه: داشر، ودانسر، وفيكسن، وكوميت، وكيوبيد، ودونر، ورودولف. سكان تينسيل لين في نونيتون، نورثامبتون، بذلوا جهدًا كبيرًا هذا العام من خلال تزيين لافتتهم مثل شجرة الميلاد. قدم ماثيو البالغ من العمر ثلاث سنوات وأمه ميشيل أورتون بالفعل لتزيين لافتة شارعهم. في غضون ذلك، سيكون فريستي هولو في نورثامبتون عنصر جذب كبير للصور الفوتوغرافية خلال هذا الشهر. ويبدو أن الأخوات نيام ديفي، البالغة من العمر ثماني سنوات، وسيارا، البالغة من العمر خمس سنوات، قد فازتا على أصدقائهما. شارع نويل، في إيلينج، لندن الغربية، يتطلب القليل من الخيال لكنه لا يزال يمكن اعتباره شارعًا يبدو مناسبًا جدًا لعيد الميلاد، إذا نطقت بشكل صحيح. ريندير كلوز أكثر وضوحًا، وسيكون من المؤكد أن سكان شرق لندن في حالة مزاجية لعيد الميلاد بحلول ديسمبر. أربع سنوات من السن بوبي جروت تلتقط صورة مع جدتها جين لونج بالقرب من ميستل تو غرين في أكسفورد. ومع ذلك، على أساس الأدلة الفوتوغرافية، لا يبدو أن المرح أكبر في لندن، حيث أن اللافتات المناسبة لعيد الميلاد أكثر شيوعًا ويتم التعامل معها بقليل من الحماس. بعض اللافتات المصورة مثل نويل رود، التي تتطلب نطقًا خاصًا، وستار رود. لكن في حالات أخرى - ريندير كلوز وهولي واي - لا يمكن الهروب من المعنى المسيحي للأسماء. قد يجذب ستريت توركيل الناس في نورث إنجلاند إلى عيد الميلاد، لكن عليهم أن يسافروا إلى تاور هاملتس لإيجاد كرانبري لين. فريستي يارد في مدينة ويستمنستر، لندن، توفر اسمًا مناسبًا لعيد الميلاد مع خلفية أكثر توراتية. يجب على أولئك الذين لا يستمتعون بعيد الميلاد أن يبتعدوا عن هذه الشوارع. وبالمثل، إذا كنت لا تشعر بالرغبة في قبلة عيد الميلاد، فتعلم من أخطاء بوبي جروت البالغة من العمر أربع سنوات، التي اشتعلت وهي تلتقط صورة مع جدتها، جين لونج، أمام ميستل غرين في أكسفورد. بالنسبة لسكان أدفنت واي، ديسمبر لا يمكن أن يأتي بسرعة كافية. على الرغم من كل شيء، لا يوجد في نويسلين في نورثلينكولن، إلينج، شارع مناسب لعيد الميلاد .', 'role': 'user'}, {'content': 'وجد بحث أجراه موظفو \"رويال ميل\" العام الماضي أن المملكة المتحدة لديها 3369 اسمًا لشوارع مُوَضوعها الميلاد. ويعد \"هولي ستريت\" أكثر أسماء الشوارع احتفالية شيوعًا في المملكة المتحدة. هناك 990 اسمًا لشوارع من هذا النوع في جميع أنحاء المملكة المتحدة.', 'role': 'assistant'}]\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "print(\"📥 Loading SFT dataset...\")\n",
        "train_dataset_sft = load_dataset(\"oddadmix/arabic-news-summarization\", split=\"train\")\n",
        "eval_dataset_sft = load_dataset(\"oddadmix/arabic-news-summarization\", split=\"test\")\n",
        "\n",
        "\n",
        "def filterEmpty(example):\n",
        "    if example[\"summary_text_translated\"] is None or example[\"origin_text_translated\"] is None:\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "train_dataset_sft = train_dataset_sft.filter(filterEmpty)\n",
        "eval_dataset_sft = eval_dataset_sft.filter(filterEmpty)\n",
        "\n",
        "\n",
        "def convert_to_conversation(example):\n",
        "    example[\"messages\"] = [\n",
        "        {\n",
        "        \"content\": \"Summarize the following text: \\n\\n \" + example[\"origin_text_translated\"] ,\n",
        "        \"role\": \"user\"\n",
        "        },\n",
        "        {\n",
        "        \"content\": example[\"summary_text_translated\"],\n",
        "        \"role\": \"assistant\"\n",
        "        }\n",
        "        ]\n",
        "    return example\n",
        "\n",
        "train_dataset_sft = train_dataset_sft.map(convert_to_conversation, remove_columns=[\"origin_text_translated\", \"origin_text\", \"summary_text_translated\",\"summary_text\"])\n",
        "eval_dataset_sft = eval_dataset_sft.map(convert_to_conversation, remove_columns=[\"origin_text_translated\", \"origin_text\", \"summary_text_translated\",\"summary_text\"])\n",
        "\n",
        "print(\"✅ SFT Dataset loaded:\")\n",
        "print(f\"   📚 Train samples: {len(train_dataset_sft)}\")\n",
        "print(f\"   🧪 Eval samples: {len(eval_dataset_sft)}\")\n",
        "print(f\"\\n📝 Single Sample: {train_dataset_sft[0]['messages']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5pI5JWpIlFQ"
      },
      "source": [
        "## Launch Training\n",
        "\n",
        "We are now ready to launch an SFT run with `SFTTrainer`, feel free to modify `SFTConfig` to play around with different configurations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ixD8Po-eAbPp",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "eeafa9bf-3976-4b17-b64c-68f4ae4ff808"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🏗️  Creating SFT trainer...\n",
            "\n",
            "🚀 Starting SFT training...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='25' max='5954' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  25/5954 01:10 < 5:01:38, 0.33 it/s, Epoch 0.00/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from trl import SFTConfig, SFTTrainer\n",
        "\n",
        "sft_config = SFTConfig(\n",
        "    output_dir=\"./lfm2-sft-summary\",\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=3,\n",
        "    learning_rate=5e-5,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    warmup_steps=100,\n",
        "    warmup_ratio=0.2,\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    report_to=None,\n",
        "    bf16=False # <- not all colab GPUs support bf16\n",
        ")\n",
        "\n",
        "print(\"🏗️  Creating SFT trainer...\")\n",
        "sft_trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=sft_config,\n",
        "    train_dataset=train_dataset_sft,\n",
        "    eval_dataset=eval_dataset_sft,\n",
        "    processing_class=tokenizer,\n",
        ")\n",
        "\n",
        "print(\"\\n🚀 Starting SFT training...\")\n",
        "sft_trainer.train()\n",
        "\n",
        "print(\"🎉 SFT training completed!\")\n",
        "\n",
        "sft_trainer.push_to_hub(\"oddadmix/arabic-summarization_test\")\n",
        "#sft_trainer.save_model()\n",
        "print(f\"💾 SFT model saved to: {sft_config.output_dir}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}